{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.rcParams.update({'font.size': 18})\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning** learning from data when we do not have (or do not use) labels. It's used to find patterns in data, and learn about the system we're studying. \n",
    "\n",
    "There are many different types of unspervised algorithms, we're going to investigate one of the standard **clustering** algorithms which is used to identify clusters in data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is used to find and group together related data. It allows machines to learn in an unsupervised way, although, often we test out unsupervised learning algorithms on datasets which do have labels, and we don't give the algorithm the labels until the end to see if the algorithm gets it correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data and Code adapted from Mubaris at \n",
    "https://github.com/mubaris/friendly-fortnight/blob/master/kmeans-from-scratch-and-sklearn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import all the libraries we need for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "#plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "# import the method KMeans from sklearr\n",
    "from sklearn.cluster import KMeans\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "#plt.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little function to make the plots later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plotter(clusters, k):\n",
    "    colors = ['r', 'g', 'b', 'y', 'c', 'm', 'lightcoral', 'darkorange', 'purple', 'darkcyan']\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(k):\n",
    "        points = np.array([X[j] for j in range(len(X)) if clusters[j] == i])\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=10, c=colors[i])\n",
    "    ax.scatter(C[:, 0], C[:, 1], marker='*', s=200, c='#050505')\n",
    "    ax.cla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas (pd) to import some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "data = pd.read_csv('./friendly-fortnight-master/xclara.csv')\n",
    "print(\"Input Data and Shape\")\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to have a look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the values and plotting it\n",
    "f1 = data['V1'].values\n",
    "f2 = data['V2'].values\n",
    "X = np.array(list(zip(f1, f2)))\n",
    "plt.scatter(f1, f2, c='black', s=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Human beings are great. I bet that you all agree with me that there are 3 clusters in that data, and you'd all draw circles around them in around the same place. \n",
    "\n",
    "How do you know which points belong to which cluster? Think of how you would define to a machine how to put the points together, it's hard isn't it? \n",
    "\n",
    "Clustering data is quite a hard problem for a computer to do. \n",
    "\n",
    "In this example, we're going to try and get a machine to cluster the data for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the distance between 2 points, we will be using this later, for now, don't worry about it. (Although if you are interested, this is a function to calculate the Eudclidian distance between two points (a and b)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance Calculator\n",
    "def dist(a, b, ax=1):\n",
    "    return np.linalg.norm(a - b, axis=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start off with k=2. Why? This data is 2-dimensional and easy to plot and see by eye that there are 3 clusters, but our data is usually of a much higher dimension and it is not always obvious what k should be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're going to use k-means to cluster the data we input, and try out several values of k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start from here if you're re-running the k-means code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters -\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sets two initial starting points, called *centroids* (as we hope the centroids will end up at the centre of the clusters). We pick them randomly, noticed that if you run the code below a few times that you get different positions. `C_x` and `C_y` are the x and y positions of the centroids as a list, we then create `C` which is column of the centroids position in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X coordinates of random centroids\n",
    "C_x = np.random.randint(0, np.max(X)-20, size=k)\n",
    "# Y coordinates of random centroids\n",
    "C_y = np.random.randint(0, np.max(X)-20, size=k)\n",
    "C = np.array(list(zip(C_x, C_y)), dtype=np.float32)\n",
    "print(\"Initial Centroids\")\n",
    "print(\"x \\t y\")\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots the data with the initial guesses for the centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting along with the Centroids\n",
    "plt.scatter(f1, f2, c='#050505', s=7)\n",
    "plt.scatter(C_x, C_y, marker='*', s=200, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We update the positions of the centroids each step.  `clusters` is a matrix of which cluster a point belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store the value of centroids when it updates\n",
    "C_old = np.zeros(C.shape)\n",
    "# Cluster Lables(0, 1, 2)\n",
    "clusters = np.zeros(len(X))\n",
    "# Error func. - Distance between new centroids and old centroids\n",
    "error = dist(C, C_old, None)\n",
    "print(\"starting error is {}\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codeblock below will loop until the error is zero and should print out the error each step and then print out all the intermediate positions of the centroids. \n",
    "\n",
    "*If the code just sits there on 0 error when you're rerunning this code, that means you've not re-initialized the starting centroids, go back and do that!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop will run till the error becomes zero\n",
    "error_list = [error]\n",
    "while error != 0:\n",
    "    # Assigning each value to its closest cluster\n",
    "    for i in range(len(X)):\n",
    "        distances = dist(X[i], C)\n",
    "        cluster = np.argmin(distances)\n",
    "        clusters[i] = cluster\n",
    "    kmeans_plotter(clusters=clusters, k=k)\n",
    "    # Storing the old centroid values\n",
    "    C_old = deepcopy(C)\n",
    "    # Finding the new centroids by taking the average value\n",
    "    for i in range(k):\n",
    "        points = [X[j] for j in range(len(X)) if clusters[j] == i]\n",
    "        C[i] = np.mean(points, axis=0)\n",
    "    error = dist(C, C_old, None)\n",
    "    print(\"Fitting error is: {}\".format(error))\n",
    "    error_list.append(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code produces a plot of the fitting error over time: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error_list, marker='o')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Training error for k = ' + str(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This plots the final state of the k-means cluster allocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plotter(clusters=clusters, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now how do we find which value of k is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it happens, KMeans is such a standard technique it is built in to sklearn, and instead of running the code by hand, we can just use the sklearn version, like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line instantiates a KMeans object we're calling kmeans. We're giving it two **parameters**, `n_clusters` and `verbose`. The arameter `n_clusters` is the number of clusters to fit, and we tell it to use `k` as the number of clusters. N.B. `k` was assigned above. `verbose` is a setting to the program, verbose=True tells it to print out everything its doing to the screen (i.e. be verbose). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=k,verbose=True)\n",
    "print('Using k of {}'.format(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now put the data `X` into kmeans and call the `.fit()` function on it. We overwrite the kmeans object with this new object that has been fitted. Note that this is equivalent to the code above which ran a certain number of times and printed out the error at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the input data\n",
    "kmeans=kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line does `.predict()` on `X` and assigns labels to each point, these labels are the numbers of which cluster a point is in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, see which cluster points 0, 500, 1000, 1500, 2000, 2500 are 3000 are in is in with this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of finding the labels for point 0, 1 and 2\n",
    "kmeans.predict(X[0:3000:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets the final centroid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroid values\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare the centroids we found above with those found from the inbuilt method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with scikit-learn centroids\n",
    "print(\"Centroid values\")\n",
    "print(\"Scratch\")\n",
    "print(C) # From Scratch\n",
    "print(\"sklearn\")\n",
    "print(centroids) # From sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaulate the k-means clusters, we calculate the sum of squared error (SSE). \n",
    "For each point, get the distance between a point x and the centroid of the cluster, then square that distance and sum for all points. \n",
    "$$\n",
    "SSE = \\sum_{i=1}^{k} \\sum_{x \\in C_i} (dist(i, centroid_c))^2\n",
    "$$\n",
    "We could simply sum the distances, but as some will be positive and some negative they could cancel out and gives us a lower measure. So we square the errors instead. \n",
    "\n",
    "Yesterday, in the regression workshop we used $R^2$ values, which is the sum of squared distances between the points and the model (i.e. the best fit line). The SSE is the same thing, except here we are care about the distance to the nearest cluster.\n",
    "\n",
    "**In the KMeans package, the SSE is given by `.inertia_`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.  \n",
    "**Re-run the code above for a k of 1, 2, 3, 4, 5 and look at the clusters that are formed. Set `SSE_1` equal the kmeans.inertia when k is 1, `SSE_2` to kmeans.inertia for k is 2 and do this for each value of k.** i.e. type SSE_1=kmeans.inertia after you run k-means for k is 1. \n",
    "Then run the following code to plot the SSE for each value of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to remind you which k you are on!\n",
    "print(\"k ={}\".format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! CHANGE THIS CODE EACH TIME YOU CHANGE K!\n",
    "SSE_2=kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list=[1,2,3,4,5]\n",
    "SSE_list=[SSE_1, SSE_2, SSE_3, SSE_4, SSE_5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_list,SSE_list, \"-o\") # this plots the data\n",
    "plt.xlabel('k') # this labels teh x axis\n",
    "plt.ylabel('SSE') # this labels the y axis\n",
    "plt.xticks(np.arange(0, 6, step=1)); # this sets the ticks to be integers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice in this plot that the rate of the SSE (sum of squared error) improvement drops off after **k=3**. Generally people use the **Elbow method** of determining k, i.e. you look for the *elbow of the plot*, namely, where the gradient suddenly reduces. This is the point where adding more clusters doesn't really gain you much in describing the data. \n",
    "\n",
    "As I said in the previous tutorials, models should be as simple as possible but no simpler. Adding more clusters complicates the model for minimal gains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, we have 3-dimensional data with 4 clusters, run the code below to make and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a sample dataset with 4 clusters\n",
    "X, y = make_blobs(n_samples=800, n_features=3, centers=4)\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will run kmeans with k=4 and hopefully find the clusters, try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing KMeans with 4 clusters (not verbose this time, unless you want to see it training)\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "# Fitting with inputs\n",
    "kmeans = kmeans.fit(X)\n",
    "# Predicting the clusters\n",
    "labels = kmeans.predict(X)\n",
    "# Getting the cluster centers\n",
    "C = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plots the data with the found clusters, hopefully you've got 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y)\n",
    "ax.scatter(C[:, 0], C[:, 1], C[:, 2], marker='*', c='#020202', s=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code runs k-means for values of k from 0 to 9, what value of k is best from this graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sse = {}\n",
    "# # this loop runs over k from 1 to 10\n",
    "for k in [1,2, 3,4,5,6,7,8,9,10]:\n",
    "    # this line does k-means on the data (fitting it), N.B. here we set up the kmeans object and fit with it in the same line\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000)\n",
    "    kmeans = kmeans.fit(X)\n",
    "    # this sets up the clusters for each point\n",
    "    #X[\"clusters\"] = kmeans.labels_\n",
    "    # this assigns the SSE for each value of k\n",
    "    sse[k] = kmeans.inertia_ \n",
    "    print(\"k is: {}, SSE is {}\".format(k, sse[k]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()),'-o')\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Do k-means on the iris dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flower](https://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise: Run the code below to load in the iris dataset, which is a set of measurements for the sepals and petals for iris flower species. And then use the Elbow method to determine how many classes there are**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris dataset - this is a standard datascience dataset used to practice with\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data into a dataframe called X\n",
    "X = pd.DataFrame(iris.data, columns=iris['feature_names'])\n",
    "X.head()\n",
    "#data = X[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)']]\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how many classes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow of a graph can be difficult to determine, so people often use other metrics, like the *gap statistic* or the *silhouette method*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means, with PCA, on the wine dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wine](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Red_and_white_wine_12-2015.jpg/800px-Red_and_white_wine_12-2015.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set that we are going to analyze in this post is a result of a chemical analysis of wines grown in a particular region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. The attributes are: Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, and Proline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference for this data: \n",
    "Forina, M. et al, PARVUS -\n",
    "An Extendible Package for Data Exploration, Classification and Correlation.\n",
    "Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno,\n",
    "16147 Genoa, Italy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the wine and put it in a dataframe called `wine_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_names = ['Class', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', \\\n",
    "              'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315',\\\n",
    "              'Proline']\n",
    "wine_data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data', names = wine_names) \n",
    "wine_df = pd.DataFrame(wine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets have a look at the data\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the first column is the code for the class (i.e. which vinyard it came from), the rest is analytical data from the lab. Let's try plotting some boxplots of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ever, the first step is to explore the data. This command will plot boxplots of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.boxplot(column=['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', \\\n",
    "              'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315',\\\n",
    "              'Proline'], rot=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that Proline and Magnesium have higher values than the rest of the analyte measurements, so this command just plots the subset so we can look at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.boxplot(column=['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Total phenols', \\\n",
    "              'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315'],\n",
    "               rot=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unspervised learning, the machine learning algorithm looks for clusters in the data, and learns without any labels. Although, often we do have the labels and we want to learn if the clusters in the data are associated with the labels or not. In this dataset, we have data from 13-dimensional data from vinyards and we want to know if we could identify which bottle of wine came from which vinyard, merely from its screening results. \n",
    "\n",
    "As we actually do have the labels however, we can plot the data with the correct labels, as in the picture below (the 'Class' column of the dataframe identifies which vinyard the wine was from). Let's see if the alcohol level of wine can be determined from its colour. and how that varies for the 2 different vinyards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_df.plot.scatter(x = 'Alcohol', y = 'Hue', c= 'Class', colormap='Set1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the hue-alcohol dimension there is definitely 3 overlapping clusters, however overall there is no correlation between the colour of the wine and the alcohol content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "What is the relationship between flavinoids and total_phenols (find out by plotting a scatter plot). Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do a KMeans of 3 clusters on Alcohol-Hue data dimensions, and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, init = 'random', max_iter = 1, random_state = 5).fit(wine_df.iloc[:,[11,1]])\n",
    "centroids_df = pd.DataFrame(kmeans.cluster_centers_, columns = list(wine_df.iloc[:,[11,1]].columns.values))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "wine_df.plot.scatter(x = 'Alcohol', y = 'Hue', c= kmeans.labels_, colormap='Set1', ax=ax, mark_right=False)\n",
    "centroids_df.plot.scatter(x = 'Alcohol', y = 'Hue', ax = ax,  s = 80, mark_right=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare the two alcohol-hue graphs, you'll se ethat although they have 3 clusters, they're not the same. Kmeans is not capable of finding overlapping clusters, so we need to describe our data in a new coordinate set where the clusters might be separated, this is where PCA can help. \n",
    "\n",
    "Furthermore, we've only looked at 2 of the 13-dimensional data, is there a good way to plot 13-dimensional data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a co-ordinate transform to **generalised coordinates** based on the **variance** in the data, this allows us to re-express the data in the new generalised coordinates, and as these try to include as much variance as possible in each coordinate, we can get an overview of the entire dataset from only plotting the data in 2-dimensional space. This allows us to visualise a high-dimensional dataset on a 2-d graph. \n",
    "\n",
    "In this technique, the new coordinates are called **principal components** and we're doing **principal component analysis** (**PCA**).\n",
    "\n",
    "A metaphor to help understand this is vibrations. If you take the Carteisan coordinates of a molecule, say, H$_2$O, as it moves around in space, you have have all the information of the motion of that molecule in what is called localised coordinates (the Cartesian coordinates in space are localised on each atom). This is equivalent to the input data in terms of the 13 analytes (i.e. Alcohol and Hue are two dimensions of the input data). As chemists, we naturally do a coordinate transform when thinking about molecules from Cartesian coordiantes to internal, chemical coordinates like bond length ($r_{O-H^{1}}$ and $r_{O-H^{2}}$) and bond angle ($\\theta_{H^{1}OH^{2}}$). A further coordinate transform chemists do easily is to the generalised coordinates of molecular vibrations, for example, we know that water has 3 vibrations: a symetric stretch of the O-H bonds, an asymetric stretch of the O-H bonds and a angle stretch of the H-O-H bond angle. When doing a coordinate transform from Cartesian coordinates to generalised coordinates we must have the same number of generalised coordinates as Cartesian coordinates. As water has 3 atoms (and there are 3 dimensions of space), we should have 9 generalised coordinates. Three of them are the vibrations mentioned above, the other 6 are the rigid body degrees of freedom, which are the movement of the entire molecule (with no internal deformation) in the $+x$, $-x$, $+y$, $-y$, $+z$ and $-z$ directions (6 generalised coordinates). Generalised coordinates are coordinates that are spread over the entire molecule, and I hope you can understand that these motions are spread over the entire molecule. \n",
    "\n",
    "If we switch coordinates to a new coordinate system, we should be able to explain the data in terms of the new coordinate system (this is coordinate transform), and you likely did this in school when you go from Cartesian coordinates to polar coordinates. For the water molecule example, we could describe a water molecule tumbling about in terms of its cartesian coordinates of the atoms, or, we could describe the same motion in terms of vibrations and rigid body degrees of freedom. As chemists, we rarely care exactly where a molecule is in space, but we do care about the vibrations as these are probed by spectroscopy, and thus we only care about the 3 vibrations. Throwing away the 6 rigid body degrees of freedom in the description of water is a form of **coarse-graining** or **approximating** the system. \n",
    "\n",
    "You can think of the principal components as the **vibrations of the dataset** and we're doing something not unlike spectroscopy to map the data. The input data analytes are the localised coordinates. We will do a coordinate transform into the generalised coordinates from principal component analysis and re-express the wine data in terms of these new coordinates. The principal components are ordered such that the first principal component captures the largest amount of variance in the dataset, and the second principal component captures the next largest amount of variance in the dataset and so on. The vibrations of a molecule with the lowest energy are the most important to understand if we want to understand a molecule's dynamics (as the lowest energy vibrations are the most common), then the second lowest vibration etc. Similarly, the first PCA is the most important to understand if we want to characterise the dataset, then the second component, and so on. This should become clear as you work through the examples below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we make a new dataframe of just the analytical data (i.e. removing the class column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=wine_df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in machine learning we **standardize** the data, we do this to remove the effects of the different scales of the features in the 'real world' which can swamp the effects of the variance of the features in the dataset, and to make learning easier. It's also the first step in doing a PCA. If we did not do this, the signal from the large-valued analyte data (Magnesium and Proline in our data) would overwhelm the signal from the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data to have a mean of ~0 and a variance of 1\n",
    "X_std = StandardScaler().fit_transform(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a PCA instance, with 13 components, as the data has 13 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA instance: pca\n",
    "pca = PCA(n_components=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do the PCA, by doing what is called a `fit_transform` on the standardized dataset, and the output is the principal components. \n",
    "\n",
    "I'm skipping over a lot of mathematics here as we're just focussing on using the technique. What we are doing is a coordinate transform of the data towards 'generalised' coordinates based on the variance in the dataset. This is done by doing a matrix diagonalisation of the standaradized data matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the PCA on the x_std matrix, and save the outputs as principalComponents\n",
    "principalComponents = pca.fit_transform(X_std)\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets plot the amount of variance explained by each principal component. Note that the output principal components are orderd by the amount of variance they explain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variances\n",
    "features = range(pca.n_components_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.xlabel('PCA features')\n",
    "plt.ylabel('variance %')\n",
    "plt.xticks(features)\n",
    "# Save components to a DataFrame\n",
    "PCA_components = pd.DataFrame(principalComponents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What was the point of this coordinate transform?*\n",
    "\n",
    "Now that we have transformed the data into 13 new coordinates we could express the data in terms of these and we would get back exactly the same data. What is often done is to throw away the principal coordinates which only explain small amounts of the variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate cumulative sum of explained variances\n",
    "cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the graph you've just made, that if we only used the first 5 principal coordinates it explains 80% of the variance in the data, so we could **coarse-grain** the system and just use a 5-dimensional **approximation** to the data instead of the full 13-dimensional one. This is often done as a **pre-processing** step in machine learning and it is called **dimensional reduction** (as you've reduced the number of dimensions in the problem). \n",
    "\n",
    "What's we're going now is visualist the dataset in the first 2 principal components, which together explain just under 60\\% of the variance in the data, and we're going to see if there are any clusters there: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've plotted the first and second components against each other. Before, we tried plotting Alcohol against Hue, which was 2 out of the 13 analytes, then we tried plotting flavinoids against total phenols, which was a separate 2 out of the 13 analytes, and neither of these had separated clusters for the 3 different vinyards. Here, we're using the new coordinates PCA_1 and PCA_2 as a coordinates to describe our data with, and have plotted them the exact same way as we did for alcohol and hue earlier. These are only 2 out of the 13 pricipal components, but they have **transformed** the wine data, and happily, it looks like there might be 3 clusters in that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's ignore the rest of the data and do a *1-dimensional k-means* on just the *first PCA coordinate* to find out if k-means finds some clusters here. Remember from the graph above, the first 1 component explains around 40\\% of the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prints out the first principal component (it prints all the components less than 1)\n",
    "PCA_components.iloc[:,:1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Aside on python slices* To get a column out of the PCA table, use something like: `PCA_components.iloc[:,:1]`. That code will grab all the columns **up to** 1, which is column 0. \n",
    "\n",
    "`PCA_components.iloc[:,:2]` will get you columns 0 and 1\n",
    "`PCA_components.iloc[:,:3]` will get you columns 0, 1, 2. \n",
    "\n",
    "To get all columns use: `PCA_components.iloc[:,:]` \n",
    "\n",
    "To get a single column, say column number 2, use the number without the colon `PCA_components.iloc[:,[2]]`. \n",
    "\n",
    "Use the box above to test these out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from a k-means using 3 clusters on the first PCA vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, using only 1-dimensional data after PCA, we have managed to get the data into 3 rough clusters that seem to fit the clusters roughly visible by eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This instantiates a KMeans object, calls it kmeans, and then does a fit with the first PCA component\n",
    "kmeans = KMeans(n_clusters=3, init = 'random', max_iter = 1, random_state = 5).fit(PCA_components.iloc[:,:1])\n",
    "# This creates a DataFrame called centroids_df that containes the cluster centers with the names of the PCA components\n",
    "centroids_df = pd.DataFrame(kmeans.cluster_centers_, columns = list(PCA_components.iloc[:,:1].columns.values))\n",
    "# The next 3 lines plots the data in terms of the first and second principal components and colors them \n",
    "# according to which cluster the datapoints were assigned to\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "labels = kmeans.predict(PCA_components.iloc[:,:1])\n",
    "ax=plt.scatter(PCA_components[0], PCA_components[1], alpha=1, c=labels, cmap='Set1', marker='D', vmax=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as we only did a 1-D k-means on the first principl component, so the k-means is separating the clusters based only on that value, i.e the value on the x-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: What happens if we add more of the PCA components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using k-means clustering with: \n",
    " \n",
    "* a. the first 2 components\n",
    "* b. the first 3 components. \n",
    "* c. all the PCA components\n",
    "\n",
    "What is the effect of adding in the extra components? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison to the actual labels (ground truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual labels for the data, note that the clusters from the single PCA, and first and second PCA k-means matched it pretty well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This plots the actual classes of the points\n",
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=1, c=wine_df['Class'], cmap='Set2', marker='D', vmax=9)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "We do not always preprocess the data using PCA, sometimes the machine learning is done on the original data and PCA is only used for plotting. \n",
    "\n",
    "Pick your favourite 1 analyte foom the original data and do a PCA with that data and plot it on a principal component graph above. Compare it to the the ground truth and and the results from using the first PCA coordinate. Which looks closer to the ground truth? \n",
    "\n",
    "Repeat with a set of 2 analytes and then with all the analytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual labels for comparison\n",
    "plt.scatter(PCA_components[0], PCA_components[1], alpha=1, c=wine_df['Class'], cmap='Set2', marker='o', vmax=9)\n",
    "plt.xlabel('PCA 1')\n",
    "plt.ylabel('PCA 2')\n",
    "#wine_df.plot.scatter(x = 'Alcohol', y = 'OD280/OD315', c= 'Class', figsize=(12,8), colormap='jet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised learning** techniques look for clusters in the data. \n",
    "\n",
    "**Principal component analysis** is an unsupervised technique based on the variance in the data, which can be used for:\n",
    " * data analysis\n",
    " * preprocessing data (before its fed into an ML algorithm)\n",
    " * visualising high-dimensional data\n",
    "\n",
    "**k-means** is an unsupervised **clustering** algorithm that can find clusters in data. It works best when the clusters are spherical and the same size and density as one another.\n",
    "\n",
    "In this tutorial we have performed k means clustering on several datasets, and use PCA as a pre-processing step to k-means, which works well, and as a visualisation technique. \n",
    "\n",
    "Note that, the algorithms do not actually ever get the labels, so it is possible that they will not cluster according to the label. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
